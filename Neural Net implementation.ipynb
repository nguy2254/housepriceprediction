{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets \n",
    "import sklearn.linear_model \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.metrics import accuracy_score\n",
    "import urllib.request as request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize random\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define activation function\n",
    "def tanh(x):\n",
    "    \"\"\"It is a hyperbolic tangent function-Tanh. \n",
    "    The function: f(x) = (1-exp(-2x))/(1+exp(-2x)).\n",
    "    Function range from -1-1, with zero centered \n",
    "    Vanishing gradient problem - saturate and kill gradient \"\"\"\n",
    "    return (np.exp(2*x) - 1)/(np.exp(2*x) +1)\n",
    "def relu(x):\n",
    "    \"\"\"Rectified Linear unit. \n",
    "    The function: f(x) = max(0,x)\n",
    "    No vaninish gradient problem\n",
    "    Should be used for hidden layers only. \n",
    "    Should not be used for output layers\"\"\"\n",
    "    return np.argmax(0,x)\n",
    "def softmax(x):\n",
    "    \"\"\"The function: f(x)= exp(x)/sum(exp(x)).\n",
    "    It turns logits into probabilitis that sum to 1.\n",
    "    Compute softmax values for each sets of scores in x\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "def sigmoid(x):\n",
    "    \"\"\"It is an activation function of form: f(x) = 1/(1+exp(-x))\n",
    "    It ranges from 0-1, with a shape S-curve\n",
    "    Vanishing gradient problem - saturate and kill gradient\n",
    "    Slow convergence \n",
    "    Output is not zero centered. \"\"\"\n",
    "    return 1/(1+exp(-x))\n",
    "\n",
    "#Define loss derivatives\n",
    "def loss_derivative(y,y_hat):\n",
    "    \"\"\"Get the loss derivative of (y_hat-y)\"\"\"\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "def softmax_loss(y, y_hat):\n",
    "    min_val = 0.000000000001\n",
    "    n = y.shape[0]\n",
    "    loss = -1/n*np.sum(y * np.log(y_hat.clip(min=min_val)))\n",
    "    return loss \n",
    "\n",
    "#Forward Propagation\n",
    "def forward_propagation(model,a0):\n",
    "    #load parameters \n",
    "    w1,b1,w2,b2,w3,b3 = model[\"w1\"], model[\"b1\"], model[\"w2\"], model[\"b2\"], model[\"w3\"], model[\"b3\"]\n",
    "    #Calculate the linear combination\n",
    "    z1 = a0.dot(w1) + b1\n",
    "    #Put it through the first activation \n",
    "    a1 = tanh(z1)\n",
    "    #Calculate the second linear combination -1st hidden \n",
    "    z2 = a1.dot(w2) + b2 \n",
    "    #Put it through the second activation - 2nd hidden layer \n",
    "    a2= tanh(z2)\n",
    "    #Calculate the third linear step \n",
    "    z3 = a2.dot(w3) + b3 \n",
    "    #Put it throught the third activation - \n",
    "    a3 = softmax(z3)\n",
    "    #Store all results in these values\n",
    "    cache = {'a0': a0, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'a3': a3, 'z3': z3}\n",
    "    return cache \n",
    "\n",
    "#Backward Propagation \n",
    "def backward_propagation(model,cache,y):\n",
    "    #Load parameter\n",
    "    w1,b1,w2,b2,w3,b3 = model[\"w1\"], model[\"b1\"], model[\"w2\"], model[\"b2\"], model[\"w3\"], model[\"b3\"]\n",
    "    #Load forward propagation results \n",
    "    a0, a1, a2, a3 = cache['a0'], cache['a1'], cache['a2'], cache['a3'] \n",
    "    #Get the number of samples \n",
    "    n = y.shape[0]\n",
    "    #Calculate the loss derivatives with respect to outputs\n",
    "    dz3 = loss_derivative(y=y,y_hat =a3)\n",
    "    #Calculate the loss derivatives with respective to the second layer weight\n",
    "    dw3 = 1/n*(a2.T).dot(dz3)\n",
    "     #Calculate the loss derivatives with respective to the second layer bias \n",
    "    db3 = 1/n*np.sum(dz3, axis=0)\n",
    "    #Calculate the loss derivatives with respective to the second layer \n",
    "    dz2 = np.multiply(dz3.dot(w3.T), tanh_derivative(a2))\n",
    "    #Calculate the loss derivatives with respective to the first layer weight \n",
    "    dw2 = 1/n *np.dot(a1.T, dz2)\n",
    "    #Calculate the loss derivatives with respective to the first layer bias \n",
    "    db2 = 1/n*np.sum(dz2, axis=0)\n",
    "    #Calculate the loss derivatives with respective to the linear \n",
    "    dz1= np.multiply(dz2.dot(w2.T), tanh_derivative(a1))\n",
    "    #Calculate the loss derivatiives with respective to the linear weight\n",
    "    dw1 = 1/n*np.dot(a0.T, dz1)\n",
    "    #Calculate the loss derivatives with respective to the linear bias\n",
    "    db1 = 1/n*np.sum(dz1,axis=0)\n",
    "    #Store the graditent \n",
    "    grads = {'dw3':dw3, 'db3': db3, 'dw2': dw2, 'db2': db2, 'dw1': dw1, 'db1': db1}\n",
    "    return grads \n",
    "#Training Phase \n",
    "def initialize_parameters(nn_input_dim, nn_hdim, nn_output_dim):\n",
    "    #First layer weights\n",
    "    w1 = 2 * np.random.randn(nn_input_dim, nn_hdim) -1 \n",
    "    #First layer bias \n",
    "    b1 = np.zeros((1,nn_hdim))\n",
    "    #Second layer weights \n",
    "    w2 = 2 * np.random.randn(nn_hdim, nn_hdim)-1\n",
    "    #Second layer bias \n",
    "    b2 = np.zeros((1,nn_hdim))\n",
    "    #Output layer weights \n",
    "    w3= 2 * np.random.randn(nn_hdim, nn_output_dim) -1 \n",
    "    #Output layer bias \n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    #Package and return model\n",
    "    model = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2,'w3':w3,'b3':b3}\n",
    "    return model\n",
    "#Update parameter\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    #Load parameter \n",
    "    w1,b1, w2, b2, w3, b3=model['w1'], model['b1'], model['w2'], model['b2'], model['w3'], model['b3']\n",
    "    #Update parameter\n",
    "    w1  -= learning_rate * grads['dw1']\n",
    "    b1  -= learning_rate * grads['db1']\n",
    "    w2  -= learning_rate * grads['dw2']\n",
    "    b2  -= learning_rate * grads['db2']\n",
    "    w3  -= learning_rate * grads['dw3']  \n",
    "    b3  -= learning_rate * grads['db3']\n",
    "    #Store and return parameters \n",
    "    model = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2, 'w3': w3, 'b3': b3}\n",
    "    return model\n",
    "\n",
    "#Predict model\n",
    "def predict(model,x):\n",
    "    #Do forward propagation\n",
    "    c = forward_propagation(model,x)\n",
    "    #Get prediction y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat \n",
    "\n",
    "#Calculate prediction accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    #Get the total number of samples\n",
    "    n = y.shapes[0]\n",
    "    #Do prediction\n",
    "    pred = predict(model,x)\n",
    "    # Transform the shape of prediction\n",
    "    pred = pred.reshape(y.shape)\n",
    "    #Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    #Calculate accuracy\n",
    "    accuracy = (n-error)/n * 100\n",
    "    return accuracy \n",
    "\n",
    "#Implement the model -training \n",
    "def train(model,X_,y_, learning_rate, epochs = 100, print_loss = False):\n",
    "    losses = []\n",
    "    #Gradient descent. Loop over epochs \n",
    "    for i in range(0, epochs):\n",
    "        #Forward propagation\n",
    "        cache = forward_propagation(model,X_)\n",
    "        #Backward propagation\n",
    "        grads = backward_propagation(model, cache, y_)\n",
    "        #Update gradient descent parameter\n",
    "        model = update_parameters(model=model, grads=grads, learning_rate = learning_rate)\n",
    "        #Print loss and accuracy for each of 10 interation\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration ', i, ':', softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration ', i, ':', accuracy_score(y_pred=y_hat, y_true = y_true)*100, ' %')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "\n",
      "   target  target__0.0  target__1.0  target__2.0  \n",
      "0     0.0            1            0            0  \n",
      "1     0.0            1            0            0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((150, 5), (150, 3))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data \n",
    "iris = datasets.load_iris()\n",
    "iris_data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "#Get dummy\n",
    "iris_data = pd.concat([iris_data, pd.get_dummies(iris_data['target'],prefix = 'target_')], axis=1)\n",
    "print(iris_data.head(2))\n",
    "#Get x and y\n",
    "y = iris_data[['target__0.0', 'target__1.0', 'target__2.0']].values\n",
    "X = iris_data.drop(['target__0.0', 'target__1.0', 'target__2.0'], axis = 1).values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration  0 : 5.410586105017118\n",
      "Accuracy after iteration  0 : 48.66666666666667  %\n",
      "Loss after iteration  100 : 0.15421731727454951\n",
      "Accuracy after iteration  100 : 97.33333333333334  %\n",
      "Loss after iteration  200 : 0.08248612410051487\n",
      "Accuracy after iteration  200 : 99.33333333333333  %\n",
      "Loss after iteration  300 : 0.05960458724151888\n",
      "Accuracy after iteration  300 : 99.33333333333333  %\n",
      "Loss after iteration  400 : 0.045236426052703484\n",
      "Accuracy after iteration  400 : 100.0  %\n",
      "Loss after iteration  500 : 0.03611350157463259\n",
      "Accuracy after iteration  500 : 100.0  %\n",
      "Loss after iteration  600 : 0.03020407758601939\n",
      "Accuracy after iteration  600 : 100.0  %\n",
      "Loss after iteration  700 : 0.02609482485170052\n",
      "Accuracy after iteration  700 : 100.0  %\n",
      "Loss after iteration  800 : 0.02306714904662381\n",
      "Accuracy after iteration  800 : 100.0  %\n",
      "Loss after iteration  900 : 0.02072924027007497\n",
      "Accuracy after iteration  900 : 100.0  %\n",
      "Loss after iteration  1000 : 0.018854763802172533\n",
      "Accuracy after iteration  1000 : 100.0  %\n",
      "Loss after iteration  1100 : 0.017309704324031477\n",
      "Accuracy after iteration  1100 : 100.0  %\n",
      "Loss after iteration  1200 : 0.016009798169613846\n",
      "Accuracy after iteration  1200 : 100.0  %\n",
      "Loss after iteration  1300 : 0.014898590914845309\n",
      "Accuracy after iteration  1300 : 100.0  %\n",
      "Loss after iteration  1400 : 0.013936385420862715\n",
      "Accuracy after iteration  1400 : 100.0  %\n",
      "Loss after iteration  1500 : 0.01309424208180328\n",
      "Accuracy after iteration  1500 : 100.0  %\n",
      "Loss after iteration  1600 : 0.012350461315125221\n",
      "Accuracy after iteration  1600 : 100.0  %\n",
      "Loss after iteration  1700 : 0.01168839667798552\n",
      "Accuracy after iteration  1700 : 100.0  %\n",
      "Loss after iteration  1800 : 0.01109503302754235\n",
      "Accuracy after iteration  1800 : 100.0  %\n",
      "Loss after iteration  1900 : 0.010560028602776474\n",
      "Accuracy after iteration  1900 : 100.0  %\n",
      "Loss after iteration  2000 : 0.01007505047619925\n",
      "Accuracy after iteration  2000 : 100.0  %\n",
      "Loss after iteration  2100 : 0.009633301996465674\n",
      "Accuracy after iteration  2100 : 100.0  %\n",
      "Loss after iteration  2200 : 0.009229179551345976\n",
      "Accuracy after iteration  2200 : 100.0  %\n",
      "Loss after iteration  2300 : 0.008858018617283976\n",
      "Accuracy after iteration  2300 : 100.0  %\n",
      "Loss after iteration  2400 : 0.008515902794148336\n",
      "Accuracy after iteration  2400 : 100.0  %\n",
      "Loss after iteration  2500 : 0.00819951811901402\n",
      "Accuracy after iteration  2500 : 100.0  %\n",
      "Loss after iteration  2600 : 0.007906040480695365\n",
      "Accuracy after iteration  2600 : 100.0  %\n",
      "Loss after iteration  2700 : 0.007633047597817801\n",
      "Accuracy after iteration  2700 : 100.0  %\n",
      "Loss after iteration  2800 : 0.007378449472871005\n",
      "Accuracy after iteration  2800 : 100.0  %\n",
      "Loss after iteration  2900 : 0.007140432914357814\n",
      "Accuracy after iteration  2900 : 100.0  %\n",
      "Loss after iteration  3000 : 0.006917416890722397\n",
      "Accuracy after iteration  3000 : 100.0  %\n",
      "Loss after iteration  3100 : 0.006708016309675908\n",
      "Accuracy after iteration  3100 : 100.0  %\n",
      "Loss after iteration  3200 : 0.0065110124128139575\n",
      "Accuracy after iteration  3200 : 100.0  %\n",
      "Loss after iteration  3300 : 0.006325328409391285\n",
      "Accuracy after iteration  3300 : 100.0  %\n",
      "Loss after iteration  3400 : 0.006150009292735766\n",
      "Accuracy after iteration  3400 : 100.0  %\n",
      "Loss after iteration  3500 : 0.005984205020768982\n",
      "Accuracy after iteration  3500 : 100.0  %\n",
      "Loss after iteration  3600 : 0.005827156421108236\n",
      "Accuracy after iteration  3600 : 100.0  %\n",
      "Loss after iteration  3700 : 0.005678183317144341\n",
      "Accuracy after iteration  3700 : 100.0  %\n",
      "Loss after iteration  3800 : 0.00553667447559993\n",
      "Accuracy after iteration  3800 : 100.0  %\n",
      "Loss after iteration  3900 : 0.005402079056474864\n",
      "Accuracy after iteration  3900 : 100.0  %\n",
      "Loss after iteration  4000 : 0.005273899308854685\n",
      "Accuracy after iteration  4000 : 100.0  %\n",
      "Loss after iteration  4100 : 0.005151684305102796\n",
      "Accuracy after iteration  4100 : 100.0  %\n",
      "Loss after iteration  4200 : 0.005035024544660109\n",
      "Accuracy after iteration  4200 : 100.0  %\n",
      "Loss after iteration  4300 : 0.004923547289415177\n",
      "Accuracy after iteration  4300 : 100.0  %\n",
      "Loss after iteration  4400 : 0.0048169125171685315\n",
      "Accuracy after iteration  4400 : 100.0  %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ba7f4c208>]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test \n",
    "model = initialize_parameters(nn_input_dim = 5, nn_hdim = 10, nn_output_dim =3)\n",
    "model, losses = train(model,X,y, learning_rate=0.03, epochs=4500, print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.33333333333333, 33.33333333333333, 30.0, 24.0, 26.0, 26.0, 26.666666666666668, 26.0, 26.0, 26.0]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
      "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
      "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0       0.808997    1.034819             -0.659563         1.224884   \n",
      "1       0.568648    0.733629             -0.820719        -0.544721   \n",
      "\n",
      "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
      "0         0.251717  0.362177                      1.847920  1.013009   \n",
      "1        -0.293321  0.406051                      1.113449  0.965242   \n",
      "\n",
      "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
      "0           1           0           0  \n",
      "1           1           0           0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((178, 13), (178, 3))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path= \"C:\\\\Users\\\\nguqu781\\\\Desktop\\\\9. Jupyter Notebook\\\\\"\n",
    "data  = pd.read_csv(path + \"neural_net_data.csv\" )\n",
    "print(data.head(2))\n",
    "# Get labels\n",
    "y = data[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "# Get inputs; we define our x and y here.\n",
    "X = data.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1).values \n",
    "X.shape, y.shape # Print shapes just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration  0 : 6.731846516271419\n",
      "Accuracy after iteration  0 : 26.40449438202247  %\n",
      "Loss after iteration  100 : 0.7580784688204679\n",
      "Accuracy after iteration  100 : 76.96629213483146  %\n",
      "Loss after iteration  200 : 0.4239592320138007\n",
      "Accuracy after iteration  200 : 86.51685393258427  %\n",
      "Loss after iteration  300 : 0.3616849980758156\n",
      "Accuracy after iteration  300 : 89.32584269662921  %\n",
      "Loss after iteration  400 : 0.30425964825889534\n",
      "Accuracy after iteration  400 : 91.57303370786516  %\n",
      "Loss after iteration  500 : 0.26780868141814645\n",
      "Accuracy after iteration  500 : 92.13483146067416  %\n",
      "Loss after iteration  600 : 0.258885832511765\n",
      "Accuracy after iteration  600 : 92.69662921348315  %\n",
      "Loss after iteration  700 : 0.2528172930860468\n",
      "Accuracy after iteration  700 : 93.25842696629213  %\n",
      "Loss after iteration  800 : 0.24691270158234113\n",
      "Accuracy after iteration  800 : 93.25842696629213  %\n",
      "Loss after iteration  900 : 0.24155460812102952\n",
      "Accuracy after iteration  900 : 93.25842696629213  %\n",
      "Loss after iteration  1000 : 0.23715316222597702\n",
      "Accuracy after iteration  1000 : 93.25842696629213  %\n",
      "Loss after iteration  1100 : 0.22741894821660602\n",
      "Accuracy after iteration  1100 : 93.82022471910112  %\n",
      "Loss after iteration  1200 : 0.22188908363562143\n",
      "Accuracy after iteration  1200 : 93.82022471910112  %\n",
      "Loss after iteration  1300 : 0.21842813303143566\n",
      "Accuracy after iteration  1300 : 94.3820224719101  %\n",
      "Loss after iteration  1400 : 0.21546969511239658\n",
      "Accuracy after iteration  1400 : 94.3820224719101  %\n",
      "Loss after iteration  1500 : 0.21281931923085726\n",
      "Accuracy after iteration  1500 : 94.3820224719101  %\n",
      "Loss after iteration  1600 : 0.21029801069577003\n",
      "Accuracy after iteration  1600 : 94.3820224719101  %\n",
      "Loss after iteration  1700 : 0.20764077595538682\n",
      "Accuracy after iteration  1700 : 94.3820224719101  %\n",
      "Loss after iteration  1800 : 0.20400116281870678\n",
      "Accuracy after iteration  1800 : 94.3820224719101  %\n",
      "Loss after iteration  1900 : 0.19234996897899478\n",
      "Accuracy after iteration  1900 : 94.3820224719101  %\n",
      "Loss after iteration  2000 : 0.18523063977392967\n",
      "Accuracy after iteration  2000 : 94.9438202247191  %\n",
      "Loss after iteration  2100 : 0.18108416624318183\n",
      "Accuracy after iteration  2100 : 94.9438202247191  %\n",
      "Loss after iteration  2200 : 0.1776465532875316\n",
      "Accuracy after iteration  2200 : 94.9438202247191  %\n",
      "Loss after iteration  2300 : 0.17475357039858178\n",
      "Accuracy after iteration  2300 : 94.9438202247191  %\n",
      "Loss after iteration  2400 : 0.17225536784862472\n",
      "Accuracy after iteration  2400 : 94.9438202247191  %\n",
      "Loss after iteration  2500 : 0.17002414248831246\n",
      "Accuracy after iteration  2500 : 94.9438202247191  %\n",
      "Loss after iteration  2600 : 0.16796320625337438\n",
      "Accuracy after iteration  2600 : 94.9438202247191  %\n",
      "Loss after iteration  2700 : 0.1660049503073623\n",
      "Accuracy after iteration  2700 : 94.9438202247191  %\n",
      "Loss after iteration  2800 : 0.1640847439720151\n",
      "Accuracy after iteration  2800 : 94.9438202247191  %\n",
      "Loss after iteration  2900 : 0.16213176361003592\n",
      "Accuracy after iteration  2900 : 94.9438202247191  %\n",
      "Loss after iteration  3000 : 0.16010008560312744\n",
      "Accuracy after iteration  3000 : 94.9438202247191  %\n",
      "Loss after iteration  3100 : 0.1579907893412711\n",
      "Accuracy after iteration  3100 : 94.9438202247191  %\n",
      "Loss after iteration  3200 : 0.15579307378776422\n",
      "Accuracy after iteration  3200 : 94.9438202247191  %\n",
      "Loss after iteration  3300 : 0.15303919746078395\n",
      "Accuracy after iteration  3300 : 94.9438202247191  %\n",
      "Loss after iteration  3400 : 0.1472591476619674\n",
      "Accuracy after iteration  3400 : 95.50561797752809  %\n",
      "Loss after iteration  3500 : 0.14416778147603035\n",
      "Accuracy after iteration  3500 : 95.50561797752809  %\n",
      "Loss after iteration  3600 : 0.13823318809427085\n",
      "Accuracy after iteration  3600 : 96.06741573033707  %\n",
      "Loss after iteration  3700 : 0.1346267613880601\n",
      "Accuracy after iteration  3700 : 96.06741573033707  %\n",
      "Loss after iteration  3800 : 0.13142754575519133\n",
      "Accuracy after iteration  3800 : 96.06741573033707  %\n",
      "Loss after iteration  3900 : 0.12943647076965475\n",
      "Accuracy after iteration  3900 : 96.06741573033707  %\n",
      "Loss after iteration  4000 : 0.12753468446866598\n",
      "Accuracy after iteration  4000 : 96.06741573033707  %\n",
      "Loss after iteration  4100 : 0.12571554909809515\n",
      "Accuracy after iteration  4100 : 96.06741573033707  %\n",
      "Loss after iteration  4200 : 0.12382023211341671\n",
      "Accuracy after iteration  4200 : 96.06741573033707  %\n",
      "Loss after iteration  4300 : 0.12095898195066387\n",
      "Accuracy after iteration  4300 : 96.06741573033707  %\n",
      "Loss after iteration  4400 : 0.11278013349030586\n",
      "Accuracy after iteration  4400 : 96.62921348314607  %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ba972bba8>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test \n",
    "np.random.seed(100)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
